{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#!pip install tensorflow==2.6.0\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, add, Permute, Conv2D, Add, \\\n",
    "    Concatenate, Multiply, LSTM, Flatten, Activation, SeparableConv2D, average\n",
    "import numpy as np\n",
    "# import tensorflow.keras.layers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, Bidirectional, Conv2D, add, TimeDistributed, GlobalAveragePooling2D, \\\n",
    "    Dropout, \\\n",
    "    Concatenate, concatenate, \\\n",
    "    Dense, GlobalMaxPooling2D, MaxPooling2D, Lambda, Add, Flatten, BatchNormalization, Activation\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.applications.densenet import DenseNet201, DenseNet121, DenseNet169\n",
    "import math\n",
    "from tensorflow.keras.layers import Input, Bidirectional, Conv2D, add, TimeDistributed, GlobalAveragePooling2D, \\\n",
    "    Dropout, \\\n",
    "    Concatenate, concatenate, \\\n",
    "    Dense, GlobalMaxPooling2D, MaxPooling2D, Lambda, Add, Layer, BatchNormalization, ReLU, AveragePooling2D, UpSampling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Multiply, \\\n",
    "    Permute, Concatenate, \\\n",
    "    Conv2D, Add, Activation, Lambda, add, BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Reshape, Dense, multiply, add, Permute, Conv2D, Add, \\\n",
    "    Concatenate, Multiply, LSTM, Flatten, Activation, SeparableConv2D, average\n",
    "import random\n",
    "import keras_preprocessing.image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-19 13:34:49.291459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 79089 MB memory:  -> device: 0, name: NVIDIA A100 80GB PCIe, pci bus id: 0000:65:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "class ConvBlock(Layer):\n",
    "\n",
    "    def __init__(self, filters=256, kernel_size=3, dilation_rate=1, **kwargs):\n",
    "        super(ConvBlock, self).__init__(**kwargs)\n",
    "\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dilation_rate = dilation_rate\n",
    "\n",
    "        self.net = Sequential([\n",
    "            Conv2D(filters, kernel_size=kernel_size, padding='same', dilation_rate=dilation_rate, use_bias=False,\n",
    "                   kernel_initializer='he_normal'),\n",
    "            BatchNormalization(),\n",
    "            ReLU()\n",
    "        ])\n",
    "\n",
    "    def call(self, X):\n",
    "        return self.net(X)\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        return {\n",
    "            **base_config,\n",
    "            \"filters\": self.filters,\n",
    "            \"kernel_size\": self.kernel_size,\n",
    "            \"dilation_rate\": self.dilation_rate,\n",
    "        }\n",
    "\n",
    "\n",
    "def AtrousSpatialPyramidPooling(X):\n",
    "    B, H, W, C = X.shape\n",
    "\n",
    "    # Image Pooling\n",
    "    image_pool = AveragePooling2D(pool_size=(H, W))(X)\n",
    "    image_pool = ConvBlock(kernel_size=1)(image_pool)\n",
    "    image_pool = UpSampling2D(size=(H // image_pool.shape[1], W // image_pool.shape[2]),\n",
    "                              )(image_pool)\n",
    "\n",
    "    # Atrous Operaions using dilation\n",
    "    conv_1 = ConvBlock(kernel_size=1, dilation_rate=1)(X)\n",
    "    conv_6 = ConvBlock(kernel_size=3, dilation_rate=6)(X)\n",
    "    conv_12 = ConvBlock(kernel_size=3, dilation_rate=12)(X)\n",
    "    conv_18 = ConvBlock(kernel_size=3, dilation_rate=18)(X)\n",
    "\n",
    "    # Combine All\n",
    "    combined = Concatenate()([image_pool, conv_1, conv_6, conv_12, conv_18])\n",
    "    processed = ConvBlock(kernel_size=1)(combined)\n",
    "\n",
    "    # Final Output\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "    \"\"\"Contains the implementation of Convolutional Block Attention Module(CBAM) block.\n",
    "    As described in https://arxiv.org/abs/1807.06521.\n",
    "    \"\"\"\n",
    "\n",
    "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
    "    cbam_feature = spatial_attention(cbam_feature)\n",
    "\n",
    "    return cbam_feature\n",
    "\n",
    "\n",
    "# improved cbam block for remote sensing\n",
    "\n",
    "def cbam_block_improved(cbam_feature, ratio=8):\n",
    "    cbam_channel = channel_attention(cbam_feature, ratio)\n",
    "    cbam_spatial = spatial_attention(cbam_feature)\n",
    "    result1 = multiply([cbam_channel, cbam_spatial])  # was multiply previously and performance was around 92.4\n",
    "    result2 = cbam_block(cbam_feature, ratio)\n",
    "    # combine both types of information\n",
    "    cbam_feature = Add()([result1, result2])  # previously it was  Add()\n",
    "    return cbam_feature\n",
    "\n",
    "\n",
    "# new attention improved\n",
    "def cbam_block_improved_new(cbam_feature, ratio=8):\n",
    "    cbam_channel = channel_attention(cbam_feature, ratio)\n",
    "    cbam_spatial = spatial_attention(cbam_feature)\n",
    "    result1 = multiply([cbam_channel, cbam_spatial])\n",
    "    result2 = cbam_block(cbam_feature, ratio)\n",
    "    # combine both types of information\n",
    "    cbam_feature = Concatenate()([result1, result2,cbam_feature])  # previously it was  Add()\n",
    "    res1 = Conv2D(filters=256, kernel_size=1, activation='relu', padding='same')(cbam_feature)\n",
    "    # res1 = BatchNormalization()(res1)\n",
    "    return res1\n",
    "\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "    input_feature = Conv2D(256, (1, 1), activation='relu')(input_feature)\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature.shape[channel_axis]\n",
    "\n",
    "    shared_layer_one = Dense(channel // ratio,\n",
    "                             activation='relu',\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "    shared_layer_two = Dense(channel,\n",
    "                             kernel_initializer='he_normal',\n",
    "                             use_bias=True,\n",
    "                             bias_initializer='zeros')\n",
    "\n",
    "    avg_pool = GlobalAveragePooling2D()(input_feature)\n",
    "    avg_pool = Reshape((1, 1, channel))(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1, 1, channel)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1, 1, channel // ratio)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    assert avg_pool.shape[1:] == (1, 1, channel)\n",
    "\n",
    "    max_pool = GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = Reshape((1, 1, channel))(max_pool)\n",
    "    assert max_pool.shape[1:] == (1, 1, channel)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    assert max_pool.shape[1:] == (1, 1, channel // ratio)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    assert max_pool.shape[1:] == (1, 1, channel)\n",
    "\n",
    "    cbam_feature = Add()([avg_pool, max_pool])\n",
    "    cbam_feature = Activation('sigmoid')(cbam_feature)\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "    # print(cbam_feature.shape)\n",
    "    # print(input_feature.shape)\n",
    "    # element wise application\n",
    "    return multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    input_feature = Conv2D(256, (1, 1), activation='relu')(input_feature)\n",
    "    kernel_size = 7\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature.shape[1]\n",
    "        cbam_feature = Permute((2, 3, 1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature.shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "\n",
    "    avg_pool = Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert avg_pool.shape[-1] == 1\n",
    "    max_pool = Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    assert max_pool.shape[-1] == 1\n",
    "    concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    assert concat.shape[-1] == 2\n",
    "    cbam_feature = Conv2D(filters=1,\n",
    "                          kernel_size=kernel_size,\n",
    "                          strides=1,\n",
    "                          padding='same',\n",
    "                          activation='sigmoid',\n",
    "                          kernel_initializer='he_normal',\n",
    "                          use_bias=False)(concat)\n",
    "    assert cbam_feature.shape[-1] == 1\n",
    "\n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = Permute((3, 1, 2))(cbam_feature)\n",
    "\n",
    "    return multiply([input_feature, cbam_feature])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_crop_img(path, grayscale=False, color_mode='rgb', target_size=None,\n",
    "                      interpolation='nearest'):\n",
    "    \"\"\"Wraps keras_preprocessing.image.utils.loag_img() and adds cropping.\n",
    "    Cropping method enumarated in interpolation\n",
    "    # Arguments\n",
    "        path: Path to image file.\n",
    "        color_mode: One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
    "            The desired image format.\n",
    "        target_size: Either `None` (default to original size)\n",
    "            or tuple of ints `(img_height, img_width)`.\n",
    "        interpolation: Interpolation and crop methods used to resample and crop the image\n",
    "            if the target size is different from that of the loaded image.\n",
    "            Methods are delimited by \":\" where first part is interpolation and second is crop\n",
    "            e.g. \"lanczos:random\".\n",
    "            Supported interpolation methods are \"nearest\", \"bilinear\", \"bicubic\", \"lanczos\",\n",
    "            \"box\", \"hamming\" By default, \"nearest\" is used.\n",
    "            Supported crop methods are \"none\", \"center\", \"random\".\n",
    "    # Returns\n",
    "        A PIL Image instance.\n",
    "    # Raises\n",
    "        ImportError: if PIL is not available.\n",
    "        ValueError: if interpolation method is not supported.\n",
    "    \"\"\"\n",
    "\n",
    "    # Decode interpolation string. Allowed Crop methods: none, center, random\n",
    "    interpolation, crop = interpolation.split(\":\") if \":\" in interpolation else (interpolation, \"none\")\n",
    "\n",
    "    if crop == \"none\":\n",
    "        return keras_preprocessing.image.utils.load_img(path,\n",
    "                                                        grayscale=grayscale,\n",
    "                                                        color_mode=color_mode,\n",
    "                                                        target_size=target_size,\n",
    "                                                        interpolation=interpolation)\n",
    "\n",
    "    # Load original size image using Keras\n",
    "    img = keras_preprocessing.image.utils.load_img(path,\n",
    "                                                   grayscale=grayscale,\n",
    "                                                   color_mode=color_mode,\n",
    "                                                   target_size=None,\n",
    "                                                   interpolation=interpolation)\n",
    "\n",
    "    # Crop fraction of total image\n",
    "    crop_fraction = 0.875\n",
    "    target_width = target_size[1]\n",
    "    target_height = target_size[0]\n",
    "\n",
    "    if target_size is not None:\n",
    "        if img.size != (target_width, target_height):\n",
    "\n",
    "            if crop not in [\"center\", \"random\"]:\n",
    "                raise ValueError('Invalid crop method {} specified.', crop)\n",
    "\n",
    "            if interpolation not in keras_preprocessing.image.utils._PIL_INTERPOLATION_METHODS:\n",
    "                raise ValueError(\n",
    "                    'Invalid interpolation method {} specified. Supported '\n",
    "                    'methods are {}'.format(interpolation,\n",
    "                                            \", \".join(\n",
    "                                                keras_preprocessing.image.utils._PIL_INTERPOLATION_METHODS.keys())))\n",
    "\n",
    "            resample = keras_preprocessing.image.utils._PIL_INTERPOLATION_METHODS[interpolation]\n",
    "\n",
    "            width, height = img.size\n",
    "\n",
    "            # Resize keeping aspect ratio\n",
    "            # result shold be no smaller than the targer size, include crop fraction overhead\n",
    "            target_size_before_crop = (target_width / crop_fraction, target_height / crop_fraction)\n",
    "            ratio = max(target_size_before_crop[0] / width, target_size_before_crop[1] / height)\n",
    "            target_size_before_crop_keep_ratio = int(width * ratio), int(height * ratio)\n",
    "            img = img.resize(target_size_before_crop_keep_ratio, resample=resample)\n",
    "\n",
    "            width, height = img.size\n",
    "\n",
    "            if crop == \"center\":\n",
    "                left_corner = int(round(width / 2)) - int(round(target_width / 2))\n",
    "                top_corner = int(round(height / 2)) - int(round(target_height / 2))\n",
    "                return img.crop((left_corner, top_corner, left_corner + target_width, top_corner + target_height))\n",
    "            elif crop == \"random\":\n",
    "                left_shift = random.randint(0, int((width - target_width)))\n",
    "                down_shift = random.randint(0, int((height - target_height)))\n",
    "                return img.crop((left_shift, down_shift, target_width + left_shift, target_height + down_shift))\n",
    "\n",
    "    return img\n",
    "\n",
    "# Monkey patch\n",
    "keras_preprocessing.image.iterator.load_img = load_and_crop_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze_excite_block(input, ratio=16):\n",
    "    ''' Create a channel-wise squeeze-excite block\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        filters: number of output filters\n",
    "    Returns: a keras tensor\n",
    "    References\n",
    "    -   [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
    "    '''\n",
    "    init = input\n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    filters = init.shape[channel_axis]\n",
    "    se_shape = (1, 1, filters)\n",
    "\n",
    "    se = GlobalAveragePooling2D()(init)\n",
    "    se = Reshape(se_shape)(se)\n",
    "    se = Dense(filters // ratio, activation='relu', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "    se = Dense(filters, activation='sigmoid', kernel_initializer='he_normal', use_bias=False)(se)\n",
    "\n",
    "    # if K.image_data_format() == 'channels_first':\n",
    "    #     se = Permute((3, 1, 2))(se)\n",
    "\n",
    "    print(init.shape)\n",
    "    print(se.shape)\n",
    "    x = multiply([init, se])\n",
    "    return x\n",
    "\n",
    "\n",
    "def spatial_squeeze_excite_block(input):\n",
    "    ''' Create a spatial squeeze-excite block\n",
    "    Args:\n",
    "        input: input tensor\n",
    "    Returns: a keras tensor\n",
    "    References\n",
    "    -   [Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)\n",
    "    '''\n",
    "\n",
    "    se = Conv2D(1, (1, 1), activation='sigmoid', use_bias=False,\n",
    "                kernel_initializer='he_normal')(input)\n",
    "\n",
    "    x = multiply([input, se])\n",
    "    return x\n",
    "\n",
    "\n",
    "# Adaptive ECA module\n",
    "def eca_module(inputs, gamma=2, b=1):\n",
    "    x = inputs\n",
    "    # t = int(abs((K.int_shape(x)[3] * gamma) // b))\n",
    "    # k = t if t % 2 else t + 1\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Reshape((1, 1, x.shape[1]))(x)\n",
    "    x = Conv2D(1, (3, 3), padding='same', kernel_initializer='he_normal', use_bias=False)(x)\n",
    "    x = Activation('sigmoid')(x)\n",
    "    x = multiply([inputs, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def EARCM_improved_TGRS(input_tensor):\n",
    "    lev_1 = Conv2D(256, (1, 1), padding='same')(input_tensor)  # 1x1 convolution and reduce channel\n",
    "    lev_2 = Conv2D(256, (2, 2), padding='same')(input_tensor)  # 1x1 convolution and reduce channel\n",
    "    lev_3 = Conv2D(256, (3, 3), padding='same')(input_tensor)  # 3x3 convolution and reduce channel\n",
    "    lev_4 = Conv2D(256, (4, 4), padding='same')(input_tensor)  # 4x4 convolution and reduce channel\n",
    "    lev_5 = Conv2D(256, (5, 5), padding='same')(input_tensor)  # 5x5 convolution and reduce channel\n",
    "\n",
    "    # CBAM modified block\n",
    "    cbam_imp = cbam_block(input_tensor) #original cbam (cbam_block) worked to produce over 94.60% accuracy\n",
    "    # proposed approach is concat\n",
    "    res1 = Concatenate()(\n",
    "        [lev_1, lev_3, lev_5,cbam_imp])  # prev, [lev_1, lev_3, lev_5]===94.40%\n",
    "\n",
    "    # convolution 1 by 1 for the fusion\n",
    "    conv = conv1by1(res1)\n",
    "\n",
    "    # # eca for the selection of interesting regions after fusion\n",
    "    eca = eca_module(conv)  # eca worked well! acc: 94.40%\n",
    "    #eca=channel_spatial_squeeze_excite(conv) # testing this although eca worked fine to some extent\n",
    "    # bn=BatchNormalization()(eca)\n",
    "    return eca\n",
    "\n",
    "\n",
    "def channel_spatial_squeeze_excite(input, ratio=16):\n",
    "    ''' Create a spatial squeeze-excite block\n",
    "    Args:\n",
    "        input: input tensor\n",
    "        filters: number of output filters\n",
    "    Returns: a keras tensor\n",
    "    References\n",
    "    -   [Squeeze and Excitation Networks](https://arxiv.org/abs/1709.01507)\n",
    "    -   [Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks](https://arxiv.org/abs/1803.02579)\n",
    "    '''\n",
    "\n",
    "    cse = squeeze_excite_block(input, ratio)\n",
    "    sse = spatial_squeeze_excite_block(input)\n",
    "\n",
    "    x = add([cse, sse])\n",
    "    return x\n",
    "\n",
    "\n",
    "# Change the number of filters to 1280 for more information or keep 2688 as it is with 1 by 1 convolution,previously 1024\n",
    "def conv1by1(input_tensor):\n",
    "    tensor = Conv2D(256, (1, 1), activation='relu')(input_tensor)  # 256-D provides around 94% acc.\n",
    "    # tensor = squeeze_excite_block(tensor)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def EACRM(input_tensor):\n",
    "    lev_1 = Conv2D(256, (1, 1))(input_tensor)  # 1x1 convolution and reduce channel\n",
    "    # print(lev_1.shape)\n",
    "    lev_2 = Conv2D(256, (3, 3))(input_tensor)  # 3x3 convolution and reduce channel\n",
    "    # lev_2= GlobalAveragePooling2D()(lev_2)\n",
    "    # print(lev_2.shape)\n",
    "    lev_3 = cbam_block_improved(input_tensor)  # CBAM modified block\n",
    "    # print(lev_3.shape)\n",
    "    # res1 = multiply([lev_1, lev_3])\n",
    "    # proposed approach is concat\n",
    "    res1 = Concatenate()(\n",
    "        [lev_1, lev_3])\n",
    "    # res1 = Add()(\n",
    "    #     [lev_1, lev_3])\n",
    "    return res1\n",
    "\n",
    "\n",
    "def custom_resnet50(model, classes):\n",
    "    invert11_ = model.get_layer('conv2_block3_out').output\n",
    "    invert1 = model.get_layer('conv3_block4_out').output\n",
    "    invert2 = model.get_layer('conv4_block6_out').output\n",
    "    invert3 = model.get_layer('conv5_block3_out').output\n",
    "\n",
    "    invert11_ = EACRM(invert11_)\n",
    "    invert1 = EACRM(invert1)\n",
    "    invert2 = EACRM(invert2)\n",
    "    invert3 = EACRM(invert3)\n",
    "\n",
    "    # ASPP\n",
    "    invert11_ = AtrousSpatialPyramidPooling(invert11_)\n",
    "    invert1 = AtrousSpatialPyramidPooling(invert1)\n",
    "    invert2 = AtrousSpatialPyramidPooling(invert2)\n",
    "    invert3 = AtrousSpatialPyramidPooling(invert3)\n",
    "\n",
    "    # GAP\n",
    "    invert11_ = GlobalAveragePooling2D()(invert11_)\n",
    "    invert1_ = GlobalAveragePooling2D()(invert1)\n",
    "    invert2_ = GlobalAveragePooling2D()(invert2)\n",
    "    invert3_ = GlobalAveragePooling2D()(invert3)\n",
    "\n",
    "    # combine all of them\n",
    "    comb = concatenate([\n",
    "        invert11_,\n",
    "        invert1_,\n",
    "        invert2_,\n",
    "        invert3_\n",
    "    ])\n",
    "    # comb = BatchNormalization()(comb)  # added to normalize\n",
    "    dense = Dense(1024, activation='relu')(comb)  # reduced the 1024->768\n",
    "    dense = Dense(768, activation='relu')(dense)  # reduced the 1024->768\n",
    "    # softmax\n",
    "    output = Dense(classes, activation='softmax')(dense)\n",
    "    model = Model(inputs=model.input, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "def sequence_layer(input):\n",
    "    print(input.shape)\n",
    "    # input= conv1by1(input)\n",
    "    units = input.shape[1] * input.shape[2]\n",
    "    reshape = Reshape((units, input.shape[3]))(input)\n",
    "    lstm_layer = LSTM(49, input_shape=(units, input.shape[3]), return_sequences=True)(reshape)\n",
    "    flatten = Flatten()(lstm_layer)\n",
    "    return flatten\n",
    "\n",
    "\n",
    "def multi_scale_msafeb(input_tensor, ratio=4):\n",
    "    print(input_tensor.shape)\n",
    "    filter = input_tensor.shape[3]\n",
    "    # p0=Modules.AtrousSpatialPyramidPooling(input_tensor), dilation=4, and groups=10, provided 93.80%\n",
    "    pool1 = Conv2D(filters=filter / ratio, kernel_size=1, activation='relu', padding='same', dilation_rate=4,\n",
    "                   groups=8)(\n",
    "        input_tensor)  # previously 12 groups, could try 16 as well for more groups\n",
    "    pool1_ = GlobalAveragePooling2D()(pool1)\n",
    "    p1 = AtrousSpatialPyramidPooling(pool1)\n",
    "    # p1 =eca_module(pool1)\n",
    "\n",
    "    pool2 = Conv2D(filters=filter / ratio, kernel_size=3, activation='relu', padding='same', dilation_rate=4,\n",
    "                   groups=8)(\n",
    "        input_tensor)\n",
    "    pool2_ = GlobalAveragePooling2D()(pool2)\n",
    "    p2 = AtrousSpatialPyramidPooling(pool2)\n",
    "    # p2=eca_module(pool2)\n",
    "\n",
    "    pool3 = Conv2D(filters=filter / ratio, kernel_size=5, activation='relu', padding='same', dilation_rate=4,\n",
    "                   groups=8)(\n",
    "        input_tensor)\n",
    "    # pool3 = MaxPooling2D((3, 3))(res1)\n",
    "    pool3_ = GlobalAveragePooling2D()(pool3)\n",
    "    p3 = AtrousSpatialPyramidPooling(pool3)\n",
    "    # p3=eca_module(pool3)\n",
    "\n",
    "    conv = concatenate([pool1, pool2, pool3, input_tensor]) #hrv1 not added before, hrv1 added in the previous version\n",
    "\n",
    "    #conv = conv1by1(conv)\n",
    "\n",
    "    earcm = EARCM_improved_TGRS(conv)  # our own attention block\n",
    "\n",
    "    bn = BatchNormalization()(earcm)\n",
    "    return bn, pool1_, pool2_, pool3_, p1, p2, p3\n",
    "\n",
    "\n",
    "def rotation_invariant(input_tensor):\n",
    "    # rotation\n",
    "    o = input_tensor\n",
    "    r1 = tf.image.rot90(input_tensor, k=1) # counterclockwise 90 degree\n",
    "    r2 = tf.image.rot90(input_tensor, k=2) # counterclockwise 180 degree\n",
    "    r3= tf.image.rot90(input_tensor,k=3) # counterclockwise 270 degree\n",
    "    r4= tf.image.rot90(input_tensor,k=-1) # clockwise 90\n",
    "    r5=tf.image.rot90(input_tensor,k=-2) # clockwise 180\n",
    "    r6=tf.image.rot90(input_tensor, k=-3) #clockwise 270\n",
    "\n",
    "\n",
    "    # msafeb\n",
    "    o_m, o_p1, o_p2, o_p3, po1, po2, po3 = multi_scale_msafeb(o)\n",
    "    r1_m, r1_p1, r1_p2, r1_p3, p11, p12, p13 = multi_scale_msafeb(r1)\n",
    "    r2_m, r2_p1, r2_p2, r2_p3, p21, p22, p23 = multi_scale_msafeb(r2)\n",
    "    r3_m, r3_p1, r3_p2, r3_p3, p31, p32, p33 = multi_scale_msafeb(r3)\n",
    "    r4_m, r4_p1, r4_p2, r4_p3, p41, p42, p43 = multi_scale_msafeb(r4)\n",
    "    r5_m, r5_p1, r5_p2, r5_p3, p51, p52, p53 = multi_scale_msafeb(r5)\n",
    "    r6_m, r6_p1, r6_p2, r6_p3, p61, p62, p63 = multi_scale_msafeb(r6)\n",
    "    \n",
    "    \n",
    "    # convert back to original format\n",
    "    r1_m=tf.image.rot90(r1_m,k=-1)\n",
    "    r2_m=tf.image.rot90(r2_m,k=-2)\n",
    "    r3_m=tf.image.rot90(r3_m,k=-3)\n",
    "    r4_m=tf.image.rot90(r4_m,k=1)\n",
    "    r5_m=tf.image.rot90(r5_m,k=2)\n",
    "    r6_m=tf.image.rot90(r6_m,k=3)\n",
    "    \n",
    "    return average([po1, p11, p21, p31, p41, p51, p61]),average([o_m, r1_m,r2_m,r3_m,r4_m,r5_m,r6_m]),average([o_p1, r1_p1,r2_p1,r3_p1,r4_p1,r5_p1,r6_p1]), average([o_p2, r1_p2,r2_p2,r3_p2,r4_p2,r5_p2,r6_p2]), average([o_p3, r1_p3,r2_p3,r3_p3,r4_p3,r5_p3,r6_p3])\n",
    "    \n",
    "\n",
    "def hor_ver_mul(input_tensor):\n",
    "    \n",
    "    # rotation\n",
    "    o = input_tensor\n",
    "    r1 = tf.image.rot90(input_tensor, k=1) # counterclockwise 90 degree\n",
    "    r2 = tf.image.rot90(input_tensor, k=2) # counterclockwise 180 degree\n",
    "    r3= tf.image.rot90(input_tensor,k=3) # counterclockwise 270 degree\n",
    "    r4= tf.image.rot90(input_tensor,k=-1) # clockwise 90\n",
    "    r5=tf.image.rot90(input_tensor,k=-2) # clockwise 180\n",
    "    r6=tf.image.rot90(input_tensor, k=-3) #clockwise 270\n",
    "    \n",
    "    #horizontal and vertical multiply\n",
    "    o_m=multi_scale_hrv_pooling(o)\n",
    "    r1_m=multi_scale_hrv_pooling(r1)\n",
    "    r2_m=multi_scale_hrv_pooling(r2)\n",
    "    r3_m=multi_scale_hrv_pooling(r3)\n",
    "    r4_m=multi_scale_hrv_pooling(r4)\n",
    "    r5_m=multi_scale_hrv_pooling(r5)\n",
    "    r6_m=multi_scale_hrv_pooling(r6)\n",
    "    \n",
    "    # revert back to the original format\n",
    "    r1_m=tf.image.rot90(r1_m, k=-1)\n",
    "    r2_m=tf.image.rot90(r2_m,k=-2)\n",
    "    r3_m=tf.image.rot90(r3_m,k=-3)\n",
    "    r4_m=tf.image.rot90(r4_m,k=1)\n",
    "    r5_m=tf.image.rot90(r5_m,k=2)\n",
    "    r6_m=tf.image.rot90(r6_m,k=3)\n",
    "    \n",
    "    con= average([o_m, r1_m, r2_m,r3_m,r4_m, r5_m, r6_m])\n",
    "#     con=conv1by1(con)\n",
    "#     con=BatchNormalization()(con)\n",
    "#     return con\n",
    "                           \n",
    "    return concatenate([o_m, r1_m, r2_m,r3_m,r4_m, r5_m, r6_m])  #average provided over 94.50+\n",
    "\n",
    "\n",
    "\n",
    "# horizontal pooling\n",
    "def hpool(k, x, tsnr):\n",
    "    hp = AveragePooling2D(pool_size=(x, k), padding='same')(tsnr)  # max pooling along ineffective\n",
    "    return hp\n",
    "\n",
    "\n",
    "def hpool_max(k, x, tsnr):\n",
    "    hp = MaxPooling2D(pool_size=(x, k), padding='same')(tsnr)  # max pooling along ineffective\n",
    "    return hp\n",
    "\n",
    "\n",
    "# vertical pooling\n",
    "def vpool(k, x, tsnr):\n",
    "    vp = AveragePooling2D(pool_size=(x, k), padding='same')(tsnr)  # max pooling along ineffective\n",
    "    return vp\n",
    "\n",
    "\n",
    "def vpool_max(k, x, tsnr):\n",
    "    vp = MaxPooling2D(pool_size=(x, k), padding='same')(tsnr)  # max pooling along ineffective\n",
    "    return vp\n",
    "\n",
    "\n",
    "# combination and use attention to capture salient information\n",
    "def hvpool(hp, vp, tnsr):\n",
    "    # matrix multiplication\n",
    "    print(hp)\n",
    "    print(vp)\n",
    "    # print(tnsr)\n",
    "    mul = multiply([vp, hp])\n",
    "    # attention layer EARCM: TODO\n",
    "    x = Activation('sigmoid')(mul)\n",
    "    #x = multiply([tnsr, x])\n",
    "    # mul_ = EACRM(mul)\n",
    "    return x\n",
    "\n",
    "\n",
    "def multi_scale_hrv_pooling(tensor):\n",
    "    scales = [1]\n",
    "    tensors = []\n",
    "    # for i in range(0, len(scales)):\n",
    "    vp = vpool(tensor.shape[1], 1, tensor)\n",
    "    hp = hpool(1, tensor.shape[2], tensor)\n",
    "    cmb = hvpool(hp, vp, tensor)\n",
    "    # tensors.append(cmb)\n",
    "    return cmb\n",
    "\n",
    "\n",
    "def multi_scale_hrv_mx_pooling(tensor):\n",
    "    scales = [1]\n",
    "    tensors = []\n",
    "    # for i in range(0, len(scales)):\n",
    "    vp = vpool_max(tensor.shape[1], 1, tensor)\n",
    "    hp = hpool_max(1, tensor.shape[2], tensor)\n",
    "    cmb = hvpool(hp, vp, tensor)\n",
    "    # tensors.append(cmb)\n",
    "    return cmb\n",
    "\n",
    "\n",
    "def multi_scale_hrv_mixed_pooling(tensor):\n",
    "    vp = multi_scale_hrv_pooling(tensor)\n",
    "    hp = multi_scale_hrv_mx_pooling(tensor)\n",
    "    cmb = average([vp, hp])\n",
    "    return cmb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning decay rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.0001\n",
    "    drop = 0.4  # in 0.5 it provided an accuracy of 80%+\n",
    "    epochs_drop = 4.0  # 5.0 gives an optimal epochs_drop\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "# rotation of the input tensor\n",
    "def rotation(input_tensor):\n",
    "    o=input_tensor\n",
    "    o_r1= tf.image.rot90(input_tensor,k=1)\n",
    "    o_r2=tf.image.rot90(input_tensor, k=2)\n",
    "    o_r3=tf.image.rot90(input_tensor,k=3)\n",
    "    o_r11= tf.image.rot90(input_tensor,k=-1)\n",
    "    o_r12=tf.image.rot90(input_tensor, k=-2)\n",
    "    o_r13=tf.image.rot90(input_tensor,k=-3)\n",
    "    \n",
    "    \n",
    "    # pass to the attention block\n",
    "    a_o=EARCM_improved_TGRS(input_tensor)\n",
    "    a_r1=EARCM_improved_TGRS(o_r1)\n",
    "    a_r2=EARCM_improved_TGRS(o_r2)\n",
    "    a_r3=EARCM_improved_TGRS(o_r3)\n",
    "    a_r11=EARCM_improved_TGRS(o_r11)\n",
    "    a_r12=EARCM_improved_TGRS(o_r12)\n",
    "    a_r13=EARCM_improved_TGRS(o_r13)\n",
    "    \n",
    "    \n",
    "    #rotate back\n",
    "    a_r1= tf.image.rot90(a_r1,k=-1)\n",
    "    a_r2=tf.image.rot90(a_r2,k=-2)\n",
    "    a_r3=tf.image.rot90(a_r3,k=-3)\n",
    "    a_r11= tf.image.rot90(a_r11,k=1)\n",
    "    a_r12=tf.image.rot90(a_r12,k=2)\n",
    "    a_r13=tf.image.rot90(a_r13,k=3)\n",
    "    \n",
    "    #average\n",
    "    return average([a_o,a_r1,a_r2,a_r3,a_r11,a_r12,a_r13])\n",
    "\n",
    "\n",
    "# single level\n",
    "def multi_scale_aspp_hrv(model, classes):\n",
    "    m_ = model.output\n",
    "    gap=GlobalAveragePooling2D()(m_)\n",
    "    seq = sequence_layer(m_)\n",
    "    \n",
    "    erv=rotation(m_)\n",
    "    erv=GlobalAveragePooling2D()(erv)\n",
    "    \n",
    "    hrv=hor_ver_mul(m_)\n",
    "    hrv=GlobalAveragePooling2D()(hrv)\n",
    "              \n",
    "    # hrv input\n",
    "    #m, p1_, p2_, p3_ = multi_scale_msafeb(m_)\n",
    "    aspp, m, p1_, p2_, p3_ = rotation_invariant(m_)\n",
    "    m = GlobalAveragePooling2D()(m)\n",
    "    aspp=GlobalAveragePooling2D()(aspp)\n",
    "    #c = concatenate([m,p1_, p2_, p3_, erv])  # [m, p1_, p2_, p3_]>>94.40+ for the fold 1\n",
    "    c = concatenate([m,p1_, p2_, p3_,hrv])\n",
    "    # # c = BatchNormalization()(m1)\n",
    "    c = Dropout(0.2)(c)\n",
    "    outputs = Dense(classes, activation='softmax')(c)\n",
    "    prop = Model(model.input, outputs)\n",
    "    return prop\n",
    "\n",
    "\n",
    "\n",
    "def train_ml(model, train_batches, valid_batches, classes):\n",
    "    m = custom_resnet50(model, classes)\n",
    "    #m = multi_scale_aspp_hrv(model, classes)\n",
    "    # m = fine_tune(model, classes)\n",
    "    # m=ensemble(classes)\n",
    "    #m=model\n",
    "    # print(m.summary())\n",
    "    # m = model\n",
    "    NUM_EPOCHS = 50\n",
    "    LEARNING_RATE = 0.0003  # for one step\n",
    "    m.compile(loss='categorical_crossentropy',  # for multiclass use categorical_crossentropy\n",
    "              # optimizer=optimizers.SGD(lr=LEARNING_RATE,momentum=0.9),\n",
    "              optimizer=optimizers.Adam(lr=LEARNING_RATE),\n",
    "              #  optimizer=optimizers.Adam(lr_schedule),\n",
    "              metrics=['acc'])\n",
    "\n",
    "    # print(model.summary())\n",
    "    # learning schedule callback\n",
    "    # es = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    lrate = LearningRateScheduler(step_decay)\n",
    "    es = EarlyStopping(monitor='val_loss', patience=5)\n",
    "    mcp_save = ModelCheckpoint('/data/gpfs/projects/punim2008/data/Fold' + '_step1.h5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "    callbacks_list = [lrate]\n",
    "\n",
    "    STEP_SIZE_TRAIN = train_batches.n // train_batches.batch_size\n",
    "    STEP_SIZE_VALID = valid_batches.n // valid_batches.batch_size\n",
    "    # lr_decay = LearningRateScheduler(schedule=lambda epoch: LEARNING_RATE * (0.9 ** epoch))\n",
    "    # callbacks_list=[es]\n",
    "    m.fit_generator(train_batches,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_batches,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    callbacks=callbacks_list\n",
    "                    )\n",
    "\n",
    "    x = m.evaluate_generator(valid_batches,\n",
    "                             steps=np.ceil(len(valid_batches)),\n",
    "                             use_multiprocessing=False,\n",
    "                             verbose=1,\n",
    "                             workers=1,\n",
    "                             )\n",
    "    #  print('Testing time:' + str(time.clock() - test_s_time) + 'secs.')\n",
    "    m.save('model-GRSL.h5')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Found 1680 images belonging to 21 classes.\n",
      "Found 420 images belonging to 21 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99114/801484584.py:97: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  m.fit_generator(train_batches,\n",
      "/apps/easybuild-2022/easybuild/software/MPI/GCC/11.3.0/OpenMPI/4.1.4/TensorFlow/2.11.0-CUDA-11.7.0/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:256: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n",
      "2024-05-19 13:36:20.862047: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8401\n",
      "2024-05-19 13:36:21.709640: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105/105 [==============================] - 88s 749ms/step - loss: 1.1701 - acc: 0.6976 - val_loss: 10.3578 - val_acc: 0.0481 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "105/105 [==============================] - 79s 756ms/step - loss: 0.1985 - acc: 0.9476 - val_loss: 10.3540 - val_acc: 0.0481 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "105/105 [==============================] - 79s 754ms/step - loss: 0.1665 - acc: 0.9399 - val_loss: 14.8075 - val_acc: 0.0505 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "105/105 [==============================] - 79s 750ms/step - loss: 0.0741 - acc: 0.9815 - val_loss: 12.4902 - val_acc: 0.0457 - lr: 4.0000e-05\n",
      "Epoch 5/50\n",
      "105/105 [==============================] - 79s 749ms/step - loss: 0.0340 - acc: 0.9917 - val_loss: 10.7206 - val_acc: 0.0505 - lr: 4.0000e-05\n",
      "Epoch 6/50\n",
      "105/105 [==============================] - 79s 754ms/step - loss: 0.0324 - acc: 0.9923 - val_loss: 6.2644 - val_acc: 0.0817 - lr: 4.0000e-05\n",
      "Epoch 7/50\n",
      "105/105 [==============================] - 79s 752ms/step - loss: 0.0181 - acc: 0.9970 - val_loss: 2.4393 - val_acc: 0.3870 - lr: 4.0000e-05\n",
      "Epoch 8/50\n",
      "105/105 [==============================] - 79s 756ms/step - loss: 0.0175 - acc: 0.9952 - val_loss: 1.6048 - val_acc: 0.5072 - lr: 1.6000e-05\n",
      "Epoch 9/50\n",
      "105/105 [==============================] - 79s 753ms/step - loss: 0.0125 - acc: 0.9988 - val_loss: 0.5770 - val_acc: 0.8293 - lr: 1.6000e-05\n",
      "Epoch 10/50\n",
      "105/105 [==============================] - 78s 747ms/step - loss: 0.0098 - acc: 0.9982 - val_loss: 0.1382 - val_acc: 0.9543 - lr: 1.6000e-05\n",
      "Epoch 11/50\n",
      "105/105 [==============================] - 79s 748ms/step - loss: 0.0078 - acc: 0.9988 - val_loss: 0.0345 - val_acc: 0.9880 - lr: 1.6000e-05\n",
      "Epoch 12/50\n",
      "105/105 [==============================] - 79s 757ms/step - loss: 0.0079 - acc: 0.9988 - val_loss: 0.0295 - val_acc: 0.9880 - lr: 6.4000e-06\n",
      "Epoch 13/50\n",
      "105/105 [==============================] - 79s 755ms/step - loss: 0.0074 - acc: 0.9994 - val_loss: 0.0358 - val_acc: 0.9928 - lr: 6.4000e-06\n",
      "Epoch 14/50\n",
      "105/105 [==============================] - 79s 752ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0380 - val_acc: 0.9904 - lr: 6.4000e-06\n",
      "Epoch 15/50\n",
      "105/105 [==============================] - 79s 753ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.0383 - val_acc: 0.9904 - lr: 6.4000e-06\n",
      "Epoch 16/50\n",
      "105/105 [==============================] - 79s 755ms/step - loss: 0.0061 - acc: 0.9994 - val_loss: 0.0356 - val_acc: 0.9904 - lr: 2.5600e-06\n",
      "Epoch 17/50\n",
      "105/105 [==============================] - 79s 753ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.0358 - val_acc: 0.9904 - lr: 2.5600e-06\n",
      "Epoch 18/50\n",
      "105/105 [==============================] - 79s 756ms/step - loss: 0.0049 - acc: 0.9994 - val_loss: 0.0413 - val_acc: 0.9880 - lr: 2.5600e-06\n",
      "Epoch 19/50\n",
      "105/105 [==============================] - 79s 754ms/step - loss: 0.0055 - acc: 0.9994 - val_loss: 0.0421 - val_acc: 0.9880 - lr: 2.5600e-06\n",
      "Epoch 20/50\n",
      "105/105 [==============================] - 79s 755ms/step - loss: 0.0048 - acc: 0.9994 - val_loss: 0.0389 - val_acc: 0.9904 - lr: 1.0240e-06\n",
      "Epoch 21/50\n",
      "105/105 [==============================] - 79s 756ms/step - loss: 0.0072 - acc: 0.9982 - val_loss: 0.0406 - val_acc: 0.9904 - lr: 1.0240e-06\n",
      "Epoch 22/50\n",
      "105/105 [==============================] - 79s 753ms/step - loss: 0.0049 - acc: 0.9994 - val_loss: 0.0377 - val_acc: 0.9904 - lr: 1.0240e-06\n",
      "Epoch 23/50\n",
      "105/105 [==============================] - 79s 750ms/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0380 - val_acc: 0.9904 - lr: 1.0240e-06\n",
      "Epoch 24/50\n",
      "105/105 [==============================] - 79s 755ms/step - loss: 0.0062 - acc: 0.9988 - val_loss: 0.0385 - val_acc: 0.9904 - lr: 4.0960e-07\n",
      "Epoch 25/50\n",
      "105/105 [==============================] - 79s 750ms/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0374 - val_acc: 0.9904 - lr: 4.0960e-07\n",
      "Epoch 26/50\n",
      "105/105 [==============================] - 79s 748ms/step - loss: 0.0042 - acc: 0.9994 - val_loss: 0.0362 - val_acc: 0.9904 - lr: 4.0960e-07\n",
      "Epoch 27/50\n",
      "105/105 [==============================] - 79s 750ms/step - loss: 0.0059 - acc: 0.9994 - val_loss: 0.0383 - val_acc: 0.9904 - lr: 4.0960e-07\n",
      "Epoch 28/50\n",
      "105/105 [==============================] - 80s 757ms/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0381 - val_acc: 0.9904 - lr: 1.6384e-07\n",
      "Epoch 29/50\n",
      "105/105 [==============================] - 79s 757ms/step - loss: 0.0041 - acc: 0.9994 - val_loss: 0.0373 - val_acc: 0.9904 - lr: 1.6384e-07\n",
      "Epoch 30/50\n",
      "105/105 [==============================] - 80s 757ms/step - loss: 0.0052 - acc: 0.9988 - val_loss: 0.0379 - val_acc: 0.9904 - lr: 1.6384e-07\n",
      "Epoch 31/50\n",
      "105/105 [==============================] - 80s 758ms/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0374 - val_acc: 0.9904 - lr: 1.6384e-07\n",
      "Epoch 32/50\n",
      "105/105 [==============================] - 79s 748ms/step - loss: 0.0040 - acc: 0.9994 - val_loss: 0.0379 - val_acc: 0.9904 - lr: 6.5536e-08\n",
      "Epoch 33/50\n",
      "105/105 [==============================] - 79s 749ms/step - loss: 0.0038 - acc: 1.0000 - val_loss: 0.0378 - val_acc: 0.9904 - lr: 6.5536e-08\n",
      "Epoch 34/50\n",
      "105/105 [==============================] - 79s 751ms/step - loss: 0.0071 - acc: 0.9988 - val_loss: 0.0363 - val_acc: 0.9904 - lr: 6.5536e-08\n",
      "Epoch 35/50\n",
      "105/105 [==============================] - 80s 766ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.0386 - val_acc: 0.9904 - lr: 6.5536e-08\n",
      "Epoch 36/50\n",
      "105/105 [==============================] - 79s 753ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0380 - val_acc: 0.9904 - lr: 2.6214e-08\n",
      "Epoch 37/50\n",
      "105/105 [==============================] - 79s 750ms/step - loss: 0.0054 - acc: 0.9994 - val_loss: 0.0383 - val_acc: 0.9904 - lr: 2.6214e-08\n",
      "Epoch 38/50\n",
      "105/105 [==============================] - 79s 750ms/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.0380 - val_acc: 0.9904 - lr: 2.6214e-08\n",
      "Epoch 39/50\n",
      "105/105 [==============================] - 79s 752ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0386 - val_acc: 0.9904 - lr: 2.6214e-08\n",
      "Epoch 40/50\n",
      "105/105 [==============================] - 79s 748ms/step - loss: 0.0044 - acc: 0.9994 - val_loss: 0.0380 - val_acc: 0.9904 - lr: 1.0486e-08\n",
      "Epoch 41/50\n",
      "105/105 [==============================] - 79s 750ms/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0377 - val_acc: 0.9904 - lr: 1.0486e-08\n",
      "Epoch 42/50\n",
      "105/105 [==============================] - 78s 746ms/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.0375 - val_acc: 0.9904 - lr: 1.0486e-08\n",
      "Epoch 43/50\n",
      "105/105 [==============================] - 78s 747ms/step - loss: 0.0052 - acc: 0.9994 - val_loss: 0.0380 - val_acc: 0.9904 - lr: 1.0486e-08\n",
      "Epoch 44/50\n",
      "105/105 [==============================] - 79s 750ms/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0377 - val_acc: 0.9904 - lr: 4.1943e-09\n",
      "Epoch 45/50\n",
      "105/105 [==============================] - 79s 751ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0373 - val_acc: 0.9904 - lr: 4.1943e-09\n",
      "Epoch 46/50\n",
      "105/105 [==============================] - 78s 747ms/step - loss: 0.0043 - acc: 0.9994 - val_loss: 0.0380 - val_acc: 0.9904 - lr: 4.1943e-09\n",
      "Epoch 47/50\n",
      "105/105 [==============================] - 78s 744ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0364 - val_acc: 0.9904 - lr: 4.1943e-09\n",
      "Epoch 48/50\n",
      "105/105 [==============================] - 79s 753ms/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0372 - val_acc: 0.9904 - lr: 1.6777e-09\n",
      "Epoch 49/50\n",
      "105/105 [==============================] - 79s 750ms/step - loss: 0.0054 - acc: 0.9994 - val_loss: 0.0374 - val_acc: 0.9904 - lr: 1.6777e-09\n",
      "Epoch 50/50\n",
      "105/105 [==============================] - 79s 751ms/step - loss: 0.0033 - acc: 1.0000 - val_loss: 0.0378 - val_acc: 0.9904 - lr: 1.6777e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99114/801484584.py:105: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
      "  x = m.evaluate_generator(valid_batches,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/27 [==============================] - 5s 170ms/step - loss: 0.0374 - acc: 0.9905\n",
      "Test loss: 0.03740796446800232\n",
      "Test accuracy: 0.9904761910438538\n",
      "[0.9904761910438538]\n",
      "The averaged accuracy is:\n",
      "\n",
      "0.9904761910438538\n",
      "The std is: \n",
      "\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "# for i in range(0, 10):\n",
    "    # print('Fold:' + str(i + 1))\n",
    "    # print(\"*\" * 100)\n",
    "    # Two steps training process\n",
    "    # Step 1\n",
    "    # # model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "    # model = m1\n",
    "#     model = DenseNet201(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "model=ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "model.trainable = True\n",
    "# Step 2\n",
    "#     model = load_model('/data/gpfs/projects/punim2008/data/Fold_step1.h5',custom_objects={'ConvBlock':ConvBlock})\n",
    "#     model.trainable = True\n",
    "#     model.summary()\n",
    "# exit()\n",
    "# data load and train\n",
    "# root_path = \"D://Jagannath_dai/AID_/2_8/\" + str(i + 1) + '/'\n",
    "root_path = \"/home/csitaula/UCM_/8_2/1/\"\n",
    "DATASET_PATH = root_path + 'train'\n",
    "test_dir = root_path + 'val'\n",
    "# DATASET_PATH = root_path/train'\n",
    "# test_dir = 'root_path/val'\n",
    "IMAGE_SIZE = (224, 224)\n",
    "data_list = os.listdir(DATASET_PATH)\n",
    "# data_list = os.listdir('D:/COVID/four_classes/splits/f4/train')\n",
    "# Delete some classes that may interfere\n",
    "print(len(data_list))\n",
    "NUM_CLASSES = len(data_list)\n",
    "BATCH_SIZE = 16  # try reducing batch size or freeze more layers if your GPU runs out of memory\n",
    "\n",
    "# Train datagen here is a preprocessor\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255,\n",
    "                                   horizontal_flip=True\n",
    "                                   )\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_batches = train_datagen.flow_from_directory(DATASET_PATH,\n",
    "                                                  target_size=IMAGE_SIZE,\n",
    "                                                  shuffle=True,\n",
    "                                                  #interpolation='lanczos:random',  # <--------- random crop\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  # subset=\"training\",\n",
    "                                                  seed=42,\n",
    "                                                  class_mode=\"categorical\"\n",
    "                                                  # For multiclass use categorical n for binary use binary\n",
    "                                                  )\n",
    "\n",
    "valid_batches = test_datagen.flow_from_directory(test_dir,\n",
    "                                                 target_size=IMAGE_SIZE,\n",
    "                                                 shuffle=True,\n",
    "                                                 batch_size=BATCH_SIZE,\n",
    "                                                 # interpolation = 'lanczos:center', # <--------- center crop\n",
    "                                                 # subset=\"validation\",\n",
    "                                                 seed=42,\n",
    "                                                 class_mode=\"categorical\"\n",
    "                                                 # For multiclass use categorical n for binary use binary\n",
    "                                                 )\n",
    "x = train_ml(model, train_batches, valid_batches, NUM_CLASSES)\n",
    "print('Test loss:', x[0])\n",
    "print('Test accuracy:', x[1])\n",
    "acc.append(x[1])\n",
    "# del x\n",
    "# del model\n",
    "#exit(0)\n",
    "\n",
    "# print the accuracy\n",
    "print(acc)\n",
    "a = np.array(acc)\n",
    "# print('Model:' + str(j))\n",
    "print('The averaged accuracy is:\\n')\n",
    "print(np.mean(a))\n",
    "print('The std is: \\n')\n",
    "print(np.std(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
